{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../output/hdn-large.2018-05-21-b1d1867-best-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../output/hdn-large.2018-05-21-b1d1867-best-model\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.import_meta_graph(path + '.meta', clear_devices=True)\n",
    "saver.restore(sess, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensors(sess):\n",
    "    x = sess.graph.get_tensor_by_name('Model/x:0')\n",
    "    logits = sess.graph.get_tensor_by_name('Model/Max:0') # should have had a name\n",
    "    lens = sess.graph.get_tensor_by_name('Model/lens:0')\n",
    "    candidates = sess.graph.get_tensor_by_name('Model/candidate_list:0')\n",
    "    \n",
    "    return x, logits, lens, candidates\n",
    "\n",
    "x, logits, lens, candidates = load_tensors(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_hdn_datasets\n",
    "from block_timer.timer import Timer\n",
    "import pickle\n",
    "\n",
    "word_vocab_path = '../output/vocab.2018-05-10-7d764e7.pkl'\n",
    "word2id = pickle.load(open(word_vocab_path, 'rb'))\n",
    "hdn_vocab_path = '../output/hdn-vocab.2018-05-18-f48a06c.pkl'\n",
    "hdn2id = pickle.load(open(hdn_vocab_path, 'rb'))\n",
    "hdn_list_vocab_path = '../output/hdn-list-vocab.2018-05-18-f48a06c.pkl'\n",
    "hdn_list2id = pickle.load(open(hdn_list_vocab_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate.wn_utils import synset2identifier\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "id2synset = {synset2identifier(ss, '30'): ss for ss in  wn.all_synsets('n')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['physical_entity.n.01', 'event.n.01', 'creation.n.02', 'structure.n.01', 'ability.n.02', 'basic_cognitive_process.n.01', 'higher_cognitive_process.n.01', 'content.n.05', 'written_communication.n.01', 'auditory_communication.n.01']\n",
      "{'event.n.01': 'survey.n.01', 'basic_cognitive_process.n.01': 'study.n.02', 'written_communication.n.01': 'report.n.01', 'ability.n.02': 'study.n.04', 'structure.n.01': 'study.n.05', 'content.n.05': 'discipline.n.01', 'creation.n.02': 'sketch.n.01', 'higher_cognitive_process.n.01': 'cogitation.n.02', 'physical_entity.n.01': 'study.n.09', 'auditory_communication.n.01': 'study.n.10'}\n",
      "{'eng-30-00644503-n': ['eng-30-00030358-n', 'eng-30-00407535-n', 'eng-30-00575741-n', 'eng-30-00633864-n', 'eng-30-00635850-n'], 'eng-30-05755883-n': ['eng-30-05752544-n'], 'eng-30-07218470-n': ['eng-30-06362953-n', 'eng-30-06470073-n'], 'eng-30-05705355-n': ['eng-30-05650329-n', 'eng-30-05650579-n', 'eng-30-05704266-n'], 'eng-30-04345028-n': ['eng-30-02735688-n', 'eng-30-04105893-n'], 'eng-30-05996646-n': ['eng-30-05999266-n'], 'eng-30-04227144-n': ['eng-30-04076846-n', 'eng-30-03234306-n'], 'eng-30-05784242-n': ['eng-30-05770926-n', 'eng-30-05784831-n', 'eng-30-05785508-n'], 'eng-30-10666464-n': ['eng-30-00002684-n', 'eng-30-00003553-n', 'eng-30-00004258-n', 'eng-30-00004475-n', 'eng-30-00007846-n', 'eng-30-10251779-n', 'eng-30-10308504-n'], 'eng-30-07048627-n': ['eng-30-07020895-n', 'eng-30-07037465-n']}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def synsets_graph_info(wn_instance, wn_version, lemma, pos):\n",
    "    \"\"\"\n",
    "    extract:\n",
    "    1. hyponym under lowest least common subsumer\n",
    "\n",
    "    :param nltk.corpus.reader.wordnet.WordNetCorpusReader wn_instance: instance\n",
    "    of nltk.corpus.reader.wordnet.WordNetCorpusReader\n",
    "    :param str wn_version: supported: '171' | '21' | '30'\n",
    "    :param str lemma: a lemma\n",
    "    :param str pos: a pos\n",
    "\n",
    "    :rtype: dict\n",
    "    :return: mapping synset_id \n",
    "        -> 'under_lcs' -> under_lcs identifier\n",
    "        -> 'path_to_under_lcs' -> [sy1_iden, sy2_iden, sy3_iden, ...]\n",
    "    \"\"\"\n",
    "    sy_id2under_lcs_info = dict()\n",
    "\n",
    "    synsets = wn_instance.synsets(lemma, pos=pos)\n",
    "\n",
    "    if len(synsets) == 1:\n",
    "        target_sy_iden = synset2identifier(synsets[0], wn_version)\n",
    "        sy_id2under_lcs_info[target_sy_iden] = {'under_lcs': None,\n",
    "                                                'path_to_under_lcs': []}\n",
    "        return sy_id2under_lcs_info\n",
    "\n",
    "    for sy1 in synsets:\n",
    "\n",
    "        target_sy_iden = synset2identifier(sy1, wn_version)\n",
    "\n",
    "        min_path_distance = 100\n",
    "        closest_lcs = None\n",
    "\n",
    "        for sy2 in synsets:\n",
    "            if sy1 != sy2:\n",
    "                lcs_s = sy1.lowest_common_hypernyms(sy2)\n",
    "                lcs = lcs_s[0]\n",
    "\n",
    "                path_distance = sy1.shortest_path_distance(lcs)\n",
    "\n",
    "                if path_distance < min_path_distance:\n",
    "                    closest_lcs = lcs\n",
    "                    min_path_distance = path_distance\n",
    "\n",
    "        under_lcs = None\n",
    "        for hypernym_path in sy1.hypernym_paths():\n",
    "            for first, second in zip(hypernym_path, hypernym_path[1:]):\n",
    "                if first == closest_lcs:\n",
    "                    under_lcs = second\n",
    "\n",
    "                    index_under_lcs = hypernym_path.index(under_lcs)\n",
    "                    path_to_under_lcs = hypernym_path[index_under_lcs + 1:-1]\n",
    "\n",
    "                    under_lcs_iden = synset2identifier(under_lcs, wn_version)\n",
    "                    path_to_under_lcs_idens = [synset2identifier(synset, wn_version)\n",
    "                                               for synset in path_to_under_lcs]\n",
    "\n",
    "                    sy_id2under_lcs_info[target_sy_iden] = {'under_lcs': under_lcs_iden,\n",
    "                                                            'under_lcs_obj' : under_lcs,\n",
    "                                                            'path_to_under_lcs': path_to_under_lcs_idens}\n",
    "\n",
    "    return sy_id2under_lcs_info\n",
    "\n",
    "def find_hdns(lemma):\n",
    "    graph_info = synsets_graph_info(wn_instance=wn,\n",
    "                                wn_version='30',\n",
    "                                lemma=lemma,\n",
    "                                pos='n')\n",
    "    hdn2synset = {info['under_lcs']: synset for synset, info in graph_info.items()}\n",
    "    hdn_list = tuple(sorted(info['under_lcs'] # sorted to avoid arbitrary order\n",
    "                        for info in graph_info.values() \n",
    "                        if info['under_lcs']))\n",
    "    return hdn_list, hdn2synset\n",
    "\n",
    "def find_path_to_hdns(lemma):\n",
    "    graph_info = synsets_graph_info(wn_instance=wn,\n",
    "                                wn_version='30',\n",
    "                                lemma=lemma,\n",
    "                                pos='n')\n",
    "    return {synset: info['path_to_under_lcs'] for synset, info in graph_info.items()}\n",
    "\n",
    "hdn_list, hdn2synset = find_hdns('study')\n",
    "print([id2synset[i].name() for i in hdn_list])\n",
    "print({id2synset[k].name(): id2synset[v].name() for k, v in hdn2synset.items()})\n",
    "print(find_path_to_hdns('study'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = word2id['<target>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal\n",
      "goal\n",
      "\twhole.n.02/cognition.n.01/location.n.01/event.n.01\n",
      "\t--> whole.n.02 (goal.n.03)\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = 'In one of these , an exploding wire device to study systems thermodynamically up to 6000 * * f and 100 atmospheres pressure , a major goal was achieved .'.lower().split()\n",
    "sentence_as_ids = [word2id.get(w) or word2id['<unkn>'] for w in sentence_tokens]\n",
    "\n",
    "target_index = len(sentence_tokens)-4\n",
    "lemma = sentence_tokens[target_index]\n",
    "sentence_as_ids[target_index] = target_id\n",
    "hdn_list, hdn2synset = find_hdns(lemma)\n",
    "feed_dict = {x: [sentence_as_ids],\n",
    "             lens: [len(sentence_as_ids)],\n",
    "             candidates: [hdn_list2id[hdn_list]]}\n",
    "target_embeddings = sess.run(logits, feed_dict=feed_dict)\n",
    "scores = [target_embeddings[0,hdn2id[hdn]] for hdn in hdn_list]\n",
    "_, best_hdn = max(zip(scores, hdn_list))\n",
    "print(lemma)\n",
    "print('\\t' + '/'.join(id2synset[hdn].name() for hdn in hdn_list))\n",
    "print('\\t--> %s (%s)' %(id2synset[best_hdn].name(), \n",
    "                        id2synset[hdn2synset[best_hdn]].name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('eng-30-00001930-n', 'eng-30-00002137-n'),\n",
       " {'eng-30-00002137-n': 'eng-30-05690269-n',\n",
       "  'eng-30-00001930-n': 'eng-30-03839795-n'})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_hdns('obstacle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
